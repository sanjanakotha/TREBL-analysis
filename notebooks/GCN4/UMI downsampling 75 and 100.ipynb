{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making 100% for time = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_2_chunks_part_1/RPTR_1_5_S2_R1_001_2_chunks_part_1.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_2_chunks_part_2/RPTR_1_5_S2_R1_001_2_chunks_part_2.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])\n",
    "\n",
    "# Group the combined data by 'RPTR_BC' and sum the 'count' values for each group\n",
    "grouped_counts = two_chunks_together.groupby('RPTR_BC', as_index=False)['count'].sum()\n",
    "grouped_counts.to_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_1_chunk/RPTR_1_5_S2_R1_001_1_chunk.fq_simple_umi_counts.tsv\", index=None, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RPTR_BC</th>\n",
       "      <th>UMI</th>\n",
       "      <th>reads</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAAAAAAAAAAAAA</td>\n",
       "      <td>AACCCGACCCGC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAAAAAAAAAAAAA</td>\n",
       "      <td>AAGCCGCCAGGC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAAAAAAAAAAAAA</td>\n",
       "      <td>AATGGCAGAACT</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAAAAAAAAAAAAA</td>\n",
       "      <td>ACATAAGCAAGA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAAAAAAAAAAAAA</td>\n",
       "      <td>ATACCTCAGGCA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8076013</th>\n",
       "      <td>TTTTTTTTGTTGGG</td>\n",
       "      <td>GAAGGGAGTGAG</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8076014</th>\n",
       "      <td>TTTTTTTTGTTTAT</td>\n",
       "      <td>GGGTGTGACATC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8076015</th>\n",
       "      <td>TTTTTTTTTGGGCG</td>\n",
       "      <td>CCGTCAACCCGC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8076016</th>\n",
       "      <td>TTTTTTTTTTGGTG</td>\n",
       "      <td>AGTAACAACGCC</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8076017</th>\n",
       "      <td>TTTTTTTTTTTGAT</td>\n",
       "      <td>AGTGGCGGTGGA</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8076018 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                RPTR_BC           UMI  reads\n",
       "0        AAAAAAAAAAAAAA  AACCCGACCCGC      1\n",
       "1        AAAAAAAAAAAAAA  AAGCCGCCAGGC      1\n",
       "2        AAAAAAAAAAAAAA  AATGGCAGAACT      1\n",
       "3        AAAAAAAAAAAAAA  ACATAAGCAAGA      1\n",
       "4        AAAAAAAAAAAAAA  ATACCTCAGGCA      1\n",
       "...                 ...           ...    ...\n",
       "8076013  TTTTTTTTGTTGGG  GAAGGGAGTGAG      1\n",
       "8076014  TTTTTTTTGTTTAT  GGGTGTGACATC      1\n",
       "8076015  TTTTTTTTTGGGCG  CCGTCAACCCGC      1\n",
       "8076016  TTTTTTTTTTGGTG  AGTAACAACGCC      1\n",
       "8076017  TTTTTTTTTTTGAT  AGTGGCGGTGGA      1\n",
       "\n",
       "[8076018 rows x 3 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_2_chunks_part_1/RPTR_1_5_S2_R1_001_2_chunks_part_1.fq_reads_per_umi.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_2_chunks_part_2/RPTR_1_5_S2_R1_001_2_chunks_part_2.fq_reads_per_umi.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])\n",
    "\n",
    "# Group the combined data by 'RPTR_BC' and sum the 'count' values for each group\n",
    "grouped_counts = two_chunks_together.groupby(['RPTR_BC', 'UMI'], as_index=False)['reads'].sum()\n",
    "grouped_counts.to_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_1_chunk/RPTR_1_5_S2_R1_001_1_chunk.fq_reads_per_umi.tsv\", index=None, sep = \"\\t\")\n",
    "grouped_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making 75% part 1 for time = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_2_chunks_part_1/RPTR_1_5_S2_R1_001_2_chunks_part_1.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_4_chunks_part_3/RPTR_1_5_S2_R1_001_4_chunks_part_3.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])\n",
    "\n",
    "# Group the combined data by 'RPTR_BC' and sum the 'count' values for each group\n",
    "grouped_counts = two_chunks_together.groupby('RPTR_BC', as_index=False)['count'].sum()\n",
    "grouped_counts.to_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_75_part_1/RPTR_1_5_S2_R1_001_75_part_1.fq_simple_umi_counts.tsv\", index=None, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_2_chunks_part_1/RPTR_1_5_S2_R1_001_2_chunks_part_1.fq_reads_per_umi.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_4_chunks_part_3/RPTR_1_5_S2_R1_001_4_chunks_part_3.fq_reads_per_umi.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])\n",
    "\n",
    "# Group the combined data by 'RPTR_BC' and 'UMI', and sum the 'reads' values for each group\n",
    "grouped_counts = two_chunks_together.groupby(['RPTR_BC', 'UMI'], as_index=False)['reads'].sum()\n",
    "\n",
    "# Save the grouped data to a new file\n",
    "grouped_counts.to_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_75_part_1/RPTR_1_5_S2_R1_001_75_part_1.fq_reads_per_umi.tsv\", index=None, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making 75% part 2 for time = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_2_chunks_part_2/RPTR_1_5_S2_R1_001_2_chunks_part_2.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_4_chunks_part_2/RPTR_1_5_S2_R1_001_4_chunks_part_2.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])\n",
    "\n",
    "# Group the combined data by 'RPTR_BC' and sum the 'count' values for each group\n",
    "grouped_counts = two_chunks_together.groupby('RPTR_BC', as_index=False)['count'].sum()\n",
    "grouped_counts.to_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_75_part_2/RPTR_1_5_S2_R1_001_75_part_2.fq_simple_umi_counts.tsv\", index=None, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making 100% for time = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_2_chunks_part_1/RPTR_1_30_S5_R1_001_2_chunks_part_1.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_2_chunks_part_2/RPTR_1_30_S5_R1_001_2_chunks_part_2.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])\n",
    "\n",
    "# Group the combined data by 'RPTR_BC' and sum the 'count' values for each group\n",
    "grouped_counts = two_chunks_together.groupby('RPTR_BC', as_index=False)['count'].sum()\n",
    "grouped_counts.to_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_1_chunk/RPTR_1_30_S5_R1_001_1_chunk.fq_simple_umi_counts.tsv\", index=None, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_2_chunks_part_1/RPTR_1_30_S5_R1_001_2_chunks_part_1.fq_reads_per_umi.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_2_chunks_part_2/RPTR_1_30_S5_R1_001_2_chunks_part_2.fq_reads_per_umi.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Convert the pandas DataFrame to a Dask DataFrame\n",
    "dask_df = dd.from_pandas(two_chunks_together, npartitions=10)\n",
    "\n",
    "# Group by 'RPTR_BC' and 'UMI', and sum the 'reads' values, then write directly to CSV\n",
    "dask_df.groupby(['RPTR_BC', 'UMI'])['reads'].sum().to_csv(\n",
    "    \"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_1_chunk/RPTR_1_30_S5_R1_001_1_chunk.fq_reads_per_umi.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index=False,\n",
    "    single_file=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making 75% part 1 for time = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_2_chunks_part_1/RPTR_1_30_S5_R1_001_2_chunks_part_1.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_4_chunks_part_3/RPTR_1_30_S5_R1_001_4_chunks_part_3.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])\n",
    "\n",
    "# Group the combined data by 'RPTR_BC' and sum the 'count' values for each group\n",
    "grouped_counts = two_chunks_together.groupby('RPTR_BC', as_index=False)['count'].sum()\n",
    "grouped_counts.to_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_75_part_1/RPTR_1_30_S5_R1_001_75_part_1.fq_simple_umi_counts.tsv\", index=None, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making 75% part 2 for time = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_2_chunks_part_2/RPTR_1_30_S5_R1_001_2_chunks_part_2.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_4_chunks_part_2/RPTR_1_30_S5_R1_001_4_chunks_part_2.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])\n",
    "\n",
    "# Group the combined data by 'RPTR_BC' and sum the 'count' values for each group\n",
    "grouped_counts = two_chunks_together.groupby('RPTR_BC', as_index=False)['count'].sum()\n",
    "grouped_counts.to_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_75_part_2/RPTR_1_30_S5_R1_001_75_part_2.fq_simple_umi_counts.tsv\", index=None, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
