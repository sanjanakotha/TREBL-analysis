{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making 100% for time = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_2_chunks_part_1/RPTR_1_5_S2_R1_001_2_chunks_part_1.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_2_chunks_part_2/RPTR_1_5_S2_R1_001_2_chunks_part_2.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])\n",
    "\n",
    "# Group the combined data by 'RPTR_BC' and sum the 'count' values for each group\n",
    "grouped_counts = two_chunks_together.groupby('RPTR_BC', as_index=False)['count'].sum()\n",
    "grouped_counts.to_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_1_chunk/RPTR_1_5_S2_R1_001_1_chunk.fq_simple_umi_counts.tsv\", index=None, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making 75% part 1 for time = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_2_chunks_part_1/RPTR_1_5_S2_R1_001_2_chunks_part_1.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_4_chunks_part_3/RPTR_1_5_S2_R1_001_4_chunks_part_3.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])\n",
    "\n",
    "# Group the combined data by 'RPTR_BC' and sum the 'count' values for each group\n",
    "grouped_counts = two_chunks_together.groupby('RPTR_BC', as_index=False)['count'].sum()\n",
    "grouped_counts.to_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_75_part_1/RPTR_1_5_S2_R1_001_75_part_1.fq_simple_umi_counts.tsv\", index=None, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making 75% part 2 for time = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_2_chunks_part_2/RPTR_1_5_S2_R1_001_2_chunks_part_2.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_4_chunks_part_2/RPTR_1_5_S2_R1_001_4_chunks_part_2.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])\n",
    "\n",
    "# Group the combined data by 'RPTR_BC' and sum the 'count' values for each group\n",
    "grouped_counts = two_chunks_together.groupby('RPTR_BC', as_index=False)['count'].sum()\n",
    "grouped_counts.to_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_75_part_2/RPTR_1_5_S2_R1_001_75_part_2.fq_simple_umi_counts.tsv\", index=None, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making 100% for time = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_2_chunks_part_1/RPTR_1_30_S5_R1_001_2_chunks_part_1.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_2_chunks_part_2/RPTR_1_30_S5_R1_001_2_chunks_part_2.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])\n",
    "\n",
    "# Group the combined data by 'RPTR_BC' and sum the 'count' values for each group\n",
    "grouped_counts = two_chunks_together.groupby('RPTR_BC', as_index=False)['count'].sum()\n",
    "grouped_counts.to_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_1_chunk/RPTR_1_30_S5_R1_001_1_chunk.fq_simple_umi_counts.tsv\", index=None, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making 75% part 1 for time = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_2_chunks_part_1/RPTR_1_30_S5_R1_001_2_chunks_part_1.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_4_chunks_part_3/RPTR_1_30_S5_R1_001_4_chunks_part_3.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])\n",
    "\n",
    "# Group the combined data by 'RPTR_BC' and sum the 'count' values for each group\n",
    "grouped_counts = two_chunks_together.groupby('RPTR_BC', as_index=False)['count'].sum()\n",
    "grouped_counts.to_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_75_part_1/RPTR_1_30_S5_R1_001_75_part_1.fq_simple_umi_counts.tsv\", index=None, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making 75% part 2 for time = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_2_chunks_part_2/RPTR_1_30_S5_R1_001_2_chunks_part_2.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_4_chunks_part_2/RPTR_1_30_S5_R1_001_4_chunks_part_2.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])\n",
    "\n",
    "# Group the combined data by 'RPTR_BC' and sum the 'count' values for each group\n",
    "grouped_counts = two_chunks_together.groupby('RPTR_BC', as_index=False)['count'].sum()\n",
    "grouped_counts.to_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_75_part_2/RPTR_1_30_S5_R1_001_75_part_2.fq_simple_umi_counts.tsv\", index=None, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
