{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making 100% for time = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ../../output/GCN4/downsampling/trebl_experiment_yeast_pool_C_umi_AD_1_5_1_chunk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"../../output/GCN4/downsampling/trebl_experiment_yeast_pool_C_umi_AD_1_5_2_chunks_part_1/AD_1_5_2_chunks_part_1.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"../../output/GCN4/downsampling/trebl_experiment_yeast_pool_C_umi_AD_1_5_2_chunks_part_2/AD_1_5_2_chunks_part_2.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])\n",
    "\n",
    "# Group the combined data by 'AD' and 'AD_BC' and sum the 'count' values for each group\n",
    "grouped_counts = two_chunks_together.groupby([\"AD\", \"AD_BC\"], as_index=False)['count'].sum()\n",
    "grouped_counts.to_csv(\"../../output/GCN4/downsampling/trebl_experiment_yeast_pool_C_umi_AD_1_5_1_chunk/AD_1_5_1_chunk.fq_simple_umi_counts.tsv\", index=None, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 100% = 50% + 50%\n",
    "\n",
    "# # Read the first chunk of data from the specified file\n",
    "# two_chunks_pt_1 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_AD_1_5_S2_R1_001_2_chunks_part_1/AD_1_5_S2_R1_001_2_chunks_part_1.fq_reads_per_umi.tsv\", sep=\"\\t\")\n",
    "\n",
    "# # Read the second chunk of data from the specified file\n",
    "# two_chunks_pt_2 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_AD_1_5_S2_R1_001_2_chunks_part_2/AD_1_5_S2_R1_001_2_chunks_part_2.fq_reads_per_umi.tsv\", sep=\"\\t\")\n",
    "\n",
    "# # Combine the two chunks of data into a single DataFrame\n",
    "# two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])\n",
    "\n",
    "# # Group the combined data by 'AD_BC' and sum the 'count' values for each group\n",
    "# grouped_counts = two_chunks_together.groupby(['AD_BC', 'UMI'], as_index=False)['reads'].sum()\n",
    "# grouped_counts.to_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_1_chunk/RPTR_1_5_S2_R1_001_1_chunk.fq_reads_per_umi.tsv\", index=None, sep = \"\\t\")\n",
    "# grouped_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making 75% part 1 for time = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir ../../output/GCN4/downsampling/trebl_experiment_yeast_pool_C_umi_AD_1_5_75_part_1/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"../../output/GCN4/downsampling/trebl_experiment_yeast_pool_C_umi_AD_1_5_2_chunks_part_1/AD_1_5_2_chunks_part_1.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"../../output/GCN4/downsampling/trebl_experiment_yeast_pool_C_umi_AD_1_5_4_chunks_part_3/AD_1_5_4_chunks_part_3.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])\n",
    "\n",
    "# Group the combined data by 'RPTR_BC' and sum the 'count' values for each group\n",
    "grouped_counts = two_chunks_together.groupby([\"AD\", \"AD_BC\"], as_index=False)['count'].sum()\n",
    "grouped_counts.to_csv(\"../../output/GCN4/downsampling/trebl_experiment_yeast_pool_C_umi_AD_1_5_75_part_1/AD_1_5_75_part_1.fq_simple_umi_counts.tsv\", index=None, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_2_chunks_part_1/RPTR_1_5_S2_R1_001_2_chunks_part_1.fq_reads_per_umi.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_4_chunks_part_3/RPTR_1_5_S2_R1_001_4_chunks_part_3.fq_reads_per_umi.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])\n",
    "\n",
    "# Group the combined data by 'RPTR_BC' and 'UMI', and sum the 'reads' values for each group\n",
    "grouped_counts = two_chunks_together.groupby(['RPTR_BC', 'UMI'], as_index=False)['reads'].sum()\n",
    "\n",
    "# Save the grouped data to a new file\n",
    "grouped_counts.to_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_75_part_1/RPTR_1_5_S2_R1_001_75_part_1.fq_reads_per_umi.tsv\", index=None, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making 75% part 2 for time = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'RPTR_BC'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m two_chunks_together \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([two_chunks_pt_1, two_chunks_pt_2])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Group the combined data by 'RPTR_BC' and sum the 'count' values for each group\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m grouped_counts \u001b[38;5;241m=\u001b[39m two_chunks_together\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRPTR_BC\u001b[39m\u001b[38;5;124m'\u001b[39m, as_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     14\u001b[0m grouped_counts\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../../output/GCN4/downsampling/trebl_experiment_yeast_pool_C_umi_AD_1_5_75_part_2/AD_1_5_75_part_2.fq_simple_umi_counts.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/frame.py:9183\u001b[0m, in \u001b[0;36mDataFrame.groupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   9180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m by \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   9181\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to supply one of \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlevel\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 9183\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameGroupBy(\n\u001b[1;32m   9184\u001b[0m     obj\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   9185\u001b[0m     keys\u001b[38;5;241m=\u001b[39mby,\n\u001b[1;32m   9186\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   9187\u001b[0m     level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   9188\u001b[0m     as_index\u001b[38;5;241m=\u001b[39mas_index,\n\u001b[1;32m   9189\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m   9190\u001b[0m     group_keys\u001b[38;5;241m=\u001b[39mgroup_keys,\n\u001b[1;32m   9191\u001b[0m     observed\u001b[38;5;241m=\u001b[39mobserved,\n\u001b[1;32m   9192\u001b[0m     dropna\u001b[38;5;241m=\u001b[39mdropna,\n\u001b[1;32m   9193\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/groupby/groupby.py:1329\u001b[0m, in \u001b[0;36mGroupBy.__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, observed, dropna)\u001b[0m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna \u001b[38;5;241m=\u001b[39m dropna\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m grouper \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1329\u001b[0m     grouper, exclusions, obj \u001b[38;5;241m=\u001b[39m get_grouper(\n\u001b[1;32m   1330\u001b[0m         obj,\n\u001b[1;32m   1331\u001b[0m         keys,\n\u001b[1;32m   1332\u001b[0m         axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m   1333\u001b[0m         level\u001b[38;5;241m=\u001b[39mlevel,\n\u001b[1;32m   1334\u001b[0m         sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m   1335\u001b[0m         observed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default \u001b[38;5;28;01melse\u001b[39;00m observed,\n\u001b[1;32m   1336\u001b[0m         dropna\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropna,\n\u001b[1;32m   1337\u001b[0m     )\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m observed \u001b[38;5;129;01mis\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m   1340\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(ping\u001b[38;5;241m.\u001b[39m_passed_categorical \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m grouper\u001b[38;5;241m.\u001b[39mgroupings):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/pandas/core/groupby/grouper.py:1043\u001b[0m, in \u001b[0;36mget_grouper\u001b[0;34m(obj, key, axis, level, sort, observed, validate, dropna)\u001b[0m\n\u001b[1;32m   1041\u001b[0m         in_axis, level, gpr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, gpr, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1043\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(gpr)\n\u001b[1;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(gpr, Grouper) \u001b[38;5;129;01mand\u001b[39;00m gpr\u001b[38;5;241m.\u001b[39mkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1045\u001b[0m     \u001b[38;5;66;03m# Add key to exclusions\u001b[39;00m\n\u001b[1;32m   1046\u001b[0m     exclusions\u001b[38;5;241m.\u001b[39madd(gpr\u001b[38;5;241m.\u001b[39mkey)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'RPTR_BC'"
     ]
    }
   ],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_2_chunks_part_2/RPTR_1_5_S2_R1_001_2_chunks_part_2.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_4_chunks_part_2/RPTR_1_5_S2_R1_001_4_chunks_part_2.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])\n",
    "\n",
    "# Group the combined data by 'RPTR_BC' and sum the 'count' values for each group\n",
    "grouped_counts = two_chunks_together.groupby('RPTR_BC', as_index=False)['count'].sum()\n",
    "grouped_counts.to_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_5_S2_R1_001_75_part_2/RPTR_1_5_S2_R1_001_75_part_2.fq_simple_umi_counts.tsv\", index=None, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making 100% for time = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_2_chunks_part_1/RPTR_1_30_S5_R1_001_2_chunks_part_1.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_2_chunks_part_2/RPTR_1_30_S5_R1_001_2_chunks_part_2.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])\n",
    "\n",
    "# Group the combined data by 'RPTR_BC' and sum the 'count' values for each group\n",
    "grouped_counts = two_chunks_together.groupby('RPTR_BC', as_index=False)['count'].sum()\n",
    "grouped_counts.to_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_1_chunk/RPTR_1_30_S5_R1_001_1_chunk.fq_simple_umi_counts.tsv\", index=None, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_2_chunks_part_1/RPTR_1_30_S5_R1_001_2_chunks_part_1.fq_reads_per_umi.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_2_chunks_part_2/RPTR_1_30_S5_R1_001_2_chunks_part_2.fq_reads_per_umi.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "# Convert the pandas DataFrame to a Dask DataFrame\n",
    "dask_df = dd.from_pandas(two_chunks_together, npartitions=10)\n",
    "\n",
    "# Group by 'RPTR_BC' and 'UMI', and sum the 'reads' values, then write directly to CSV\n",
    "dask_df.groupby(['RPTR_BC', 'UMI'])['reads'].sum().to_csv(\n",
    "    \"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_1_chunk/RPTR_1_30_S5_R1_001_1_chunk.fq_reads_per_umi.tsv\",\n",
    "    sep=\"\\t\",\n",
    "    index=False,\n",
    "    single_file=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making 75% part 1 for time = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_2_chunks_part_1/RPTR_1_30_S5_R1_001_2_chunks_part_1.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_4_chunks_part_3/RPTR_1_30_S5_R1_001_4_chunks_part_3.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])\n",
    "\n",
    "# Group the combined data by 'RPTR_BC' and sum the 'count' values for each group\n",
    "grouped_counts = two_chunks_together.groupby('RPTR_BC', as_index=False)['count'].sum()\n",
    "grouped_counts.to_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_75_part_1/RPTR_1_30_S5_R1_001_75_part_1.fq_simple_umi_counts.tsv\", index=None, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making 75% part 2 for time = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100% = 50% + 50%\n",
    "\n",
    "# Read the first chunk of data from the specified file\n",
    "two_chunks_pt_1 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_2_chunks_part_2/RPTR_1_30_S5_R1_001_2_chunks_part_2.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Read the second chunk of data from the specified file\n",
    "two_chunks_pt_2 = pd.read_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_4_chunks_part_2/RPTR_1_30_S5_R1_001_4_chunks_part_2.fq_simple_umi_counts.tsv\", sep=\"\\t\")\n",
    "\n",
    "# Combine the two chunks of data into a single DataFrame\n",
    "two_chunks_together = pd.concat([two_chunks_pt_1, two_chunks_pt_2])\n",
    "\n",
    "# Group the combined data by 'RPTR_BC' and sum the 'count' values for each group\n",
    "grouped_counts = two_chunks_together.groupby('RPTR_BC', as_index=False)['count'].sum()\n",
    "grouped_counts.to_csv(\"/Volumes/Expansion/GCN4_TREBL/downsampling/trebl_experiment_yeast_pool_C_umi_RPTR_1_30_S5_R1_001_75_part_2/RPTR_1_30_S5_R1_001_75_part_2.fq_simple_umi_counts.tsv\", index=None, sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
